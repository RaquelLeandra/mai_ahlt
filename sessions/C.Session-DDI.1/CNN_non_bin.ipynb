{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "output_path_name = \"task9.2_raquel_6.txt\"\n",
    "\n",
    "output_path = \"evaluations/\" + output_path_name\n",
    "results_path = output_path.replace('.txt', '_All_scores.log')\n",
    "datadir = '../../data/Test-DDI/DrugBank'\n",
    "training_data = '/home/raquel/Documents/mai/ahlt/data/Train/All'\n",
    "train_df_path = '/home/raquel/Documents/mai/ahlt/data/DF/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('saved_train_nice.csv', index_col=0)\n",
    "\n",
    "sentences = train_df.sentence_text.values\n",
    "y = train_df['relation_type'].values\n",
    "\n",
    "y_binary = ['none' if i == 'none' else 'interaction 'for i in y_train]\n",
    "\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000,stratify=y)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "max_s = [len(x) for x in X_train]\n",
    "maxlen = np.max(max_s)\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kimCNN(EMBEDDING_DIM, MAX_SEQUENCE_LENGTH, MAX_NB_WORDS, word_index, num_labels,loss='binary_crossentropy'):\n",
    "    \"\"\"\n",
    "    Convolution neural network model for sentence classification.\n",
    "    Parameters\n",
    "    ----------\n",
    "    EMBEDDING_DIM: Dimension of the embedding space.\n",
    "    MAX_SEQUENCE_LENGTH: Maximum length of the sentence.\n",
    "    MAX_NB_WORDS: Maximum number of words in the vocabulary.\n",
    "    embeddings_index: A dict containing words and their embeddings.\n",
    "    word_index: A dict containing words and their indices.\n",
    "    labels_index: A dict containing the labels and their indices.\n",
    "    Returns\n",
    "    -------\n",
    "    compiled keras model\n",
    "    \"\"\"\n",
    "    print('Preparing embedding matrix.')\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = maxlen\n",
    "    num_words = vocab_size\n",
    "    embedding_layer = Embedding(input_dim=num_words,\n",
    "                                output_dim=EMBEDDING_DIM,\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "\n",
    "    print('Training model.')\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    print(embedded_sequences.shape)\n",
    "\n",
    "\n",
    "    # add first conv filter\n",
    "    embedded_sequences = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n",
    "    x = Conv2D(100, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling2D((MAX_SEQUENCE_LENGTH - 5 + 1, 1))(x)\n",
    "\n",
    "\n",
    "    # add second conv filter.\n",
    "    y = Conv2D(100, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    y = MaxPooling2D((MAX_SEQUENCE_LENGTH - 4 + 1, 1))(y)\n",
    "\n",
    "\n",
    "    # add third conv filter.\n",
    "    z = Conv2D(100, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    z = MaxPooling2D((MAX_SEQUENCE_LENGTH - 3 + 1, 1))(z)\n",
    "\n",
    "\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x,y,z])\n",
    "\n",
    "    # flatted the pooled features.\n",
    "    alpha = Flatten()(alpha)\n",
    "\n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "\n",
    "    # predictions\n",
    "    preds = Dense(num_labels, activation='softmax')(alpha)\n",
    "\n",
    "    # build model\n",
    "    model = Model(sequence_input, preds)\n",
    "    adadelta = optimizers.Adadelta()\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adadelta,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n",
      "(?, 97, 200)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 97)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 97, 200)      992200      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 97, 200, 1)   0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 93, 1, 100)   100100      reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 94, 1, 100)   80100       reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 95, 1, 100)   60100       reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 1, 300)    0           max_pooling2d_19[0][0]           \n",
      "                                                                 max_pooling2d_20[0][0]           \n",
      "                                                                 max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 300)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 300)          0           flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 5)            1505        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,234,005\n",
      "Trainable params: 1,234,005\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 20843 samples, validate on 6948 samples\n",
      "Epoch 1/30\n",
      "20843/20843 [==============================] - 26s 1ms/step - loss: 0.5790 - acc: 0.8520 - val_loss: 0.5417 - val_acc: 0.8554\n",
      "Epoch 2/30\n",
      "20843/20843 [==============================] - 30s 1ms/step - loss: 0.4759 - acc: 0.8564 - val_loss: 0.4557 - val_acc: 0.8591\n",
      "Epoch 3/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.4078 - acc: 0.8639 - val_loss: 0.3836 - val_acc: 0.8633\n",
      "Epoch 4/30\n",
      "20843/20843 [==============================] - 25s 1ms/step - loss: 0.3651 - acc: 0.8690 - val_loss: 0.3563 - val_acc: 0.8663\n",
      "Epoch 5/30\n",
      "20843/20843 [==============================] - 25s 1ms/step - loss: 0.3356 - acc: 0.8778 - val_loss: 0.3331 - val_acc: 0.8787\n",
      "Epoch 6/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.3114 - acc: 0.8833 - val_loss: 0.3381 - val_acc: 0.8624\n",
      "Epoch 7/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.2888 - acc: 0.8914 - val_loss: 0.3115 - val_acc: 0.8821\n",
      "Epoch 8/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.2731 - acc: 0.8958 - val_loss: 0.3094 - val_acc: 0.8752\n",
      "Epoch 9/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.2546 - acc: 0.9035 - val_loss: 0.3306 - val_acc: 0.8656\n",
      "Epoch 10/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.2416 - acc: 0.9078 - val_loss: 0.3000 - val_acc: 0.8886\n",
      "Epoch 11/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.2330 - acc: 0.9119 - val_loss: 0.3008 - val_acc: 0.8856\n",
      "Epoch 12/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.2267 - acc: 0.9123 - val_loss: 0.2963 - val_acc: 0.8831\n",
      "Epoch 13/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.2193 - acc: 0.9153 - val_loss: 0.3085 - val_acc: 0.8919\n",
      "Epoch 14/30\n",
      "20843/20843 [==============================] - 26s 1ms/step - loss: 0.2078 - acc: 0.9200 - val_loss: 0.3018 - val_acc: 0.8887\n",
      "Epoch 15/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1992 - acc: 0.9221 - val_loss: 0.3089 - val_acc: 0.8908\n",
      "Epoch 16/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.1955 - acc: 0.9242 - val_loss: 0.3047 - val_acc: 0.8898\n",
      "Epoch 17/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.1926 - acc: 0.9251 - val_loss: 0.3056 - val_acc: 0.8846\n",
      "Epoch 18/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1874 - acc: 0.9276 - val_loss: 0.3154 - val_acc: 0.8890\n",
      "Epoch 19/30\n",
      "20843/20843 [==============================] - 25s 1ms/step - loss: 0.1854 - acc: 0.9285 - val_loss: 0.3115 - val_acc: 0.8915\n",
      "Epoch 20/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.1782 - acc: 0.9322 - val_loss: 0.3143 - val_acc: 0.8910\n",
      "Epoch 21/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1804 - acc: 0.9281 - val_loss: 0.3270 - val_acc: 0.8896\n",
      "Epoch 22/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.1774 - acc: 0.9299 - val_loss: 0.3135 - val_acc: 0.8883\n",
      "Epoch 23/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1767 - acc: 0.9320 - val_loss: 0.3117 - val_acc: 0.8866\n",
      "Epoch 24/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1716 - acc: 0.9316 - val_loss: 0.3251 - val_acc: 0.8903\n",
      "Epoch 25/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.1673 - acc: 0.9359 - val_loss: 0.3256 - val_acc: 0.8813\n",
      "Epoch 26/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1689 - acc: 0.9326 - val_loss: 0.3192 - val_acc: 0.8906\n",
      "Epoch 27/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.1644 - acc: 0.9360 - val_loss: 0.3238 - val_acc: 0.8853\n",
      "Epoch 28/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1623 - acc: 0.9368 - val_loss: 0.3312 - val_acc: 0.8922\n",
      "Epoch 29/30\n",
      "20843/20843 [==============================] - 23s 1ms/step - loss: 0.1631 - acc: 0.9341 - val_loss: 0.3251 - val_acc: 0.8910\n",
      "Epoch 30/30\n",
      "20843/20843 [==============================] - 24s 1ms/step - loss: 0.1589 - acc: 0.9362 - val_loss: 0.3259 - val_acc: 0.8877\n",
      "[0.54210526 0.65006227 0.55421687 0.51242236 0.93709328]\n",
      "0.6391800065729827\n",
      "0.674139944176747\n",
      "0.6105644797299666\n"
     ]
    }
   ],
   "source": [
    "word_embedding_size = 200\n",
    "word_pos = 20\n",
    "model = kimCNN(EMBEDDING_DIM=word_embedding_size, MAX_SEQUENCE_LENGTH=word_pos, MAX_NB_WORDS=len(word_index), word_index=word_index, num_labels=5)\n",
    "\n",
    "\n",
    "def classify_keras(model):\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    y_train_encoded = encoder.fit_transform(y_train)\n",
    "    y_test_encoded = encoder.fit_transform(y_test)\n",
    "    model.fit(X_train, y_train_encoded,\n",
    "                    epochs=30,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test_encoded),\n",
    "                    batch_size=100,\n",
    "                    class_weight = 'auto')\n",
    "    y_pred = model.predict(X_test)\n",
    "#     y_class = y_pred > 0.5\n",
    "    y_class = np.argmax(y_pred,axis=1)\n",
    "#     y_class = y_class.astype(int)\n",
    "    y_labels = [encoder.classes_[l] for l in y_class]\n",
    "    print(f1_score(y_test, y_labels, average=None))\n",
    "    print(f1_score(y_test, y_labels, average=\"macro\"))\n",
    "    print(precision_score(y_test, y_labels, average=\"macro\"))\n",
    "    print(recall_score(y_test, y_labels, average=\"macro\"))\n",
    "\n",
    "classify_keras(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The use of drugs that stimulate alpha-adrenergic receptors (e.g., phenylephrine, pseudoephedrine, ephedrine, phenylpropanolamine or dihydroergotamine) may enhance or potentiate the pressor effects of ProAmatine   . Therefore, caution should be used when ProAmatine    is administered concomitantly with agents that cause vasoconstriction. '\n",
    "\n",
    "sentence_array = tokenizer.texts_to_sequences([sentence])\n",
    "sentence_array= pad_sequences(sentence_array, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,   59,    2,    5,   69, 2351, 3419,  592, 1845,  484,  385,\n",
       "       1675, 2319, 4867,    8,  573,   16,  426,    8,  310,    3, 1855,\n",
       "         83,    2,  715,  398,  152,    7, 2364,  161, 2506,  258,    6,\n",
       "         15,   69,  351, 1820,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = model.predict(sentence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = np.argmax(y_probs, axis=1)\n",
    "y_pred = encoder.classes_[y_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'effect'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahlt",
   "language": "python",
   "name": "ahlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
