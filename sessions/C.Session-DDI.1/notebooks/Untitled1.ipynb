{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded preprocessed data\n",
      "Preparing embedding matrix.\n",
      "Training model.\n",
      "(?, 97, 200)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 97)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 97, 200)      972800      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 97, 200, 1)   0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 93, 1, 100)   100100      reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 94, 1, 100)   80100       reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 95, 1, 100)   60100       reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 1, 300)    0           max_pooling2d_19[0][0]           \n",
      "                                                                 max_pooling2d_20[0][0]           \n",
      "                                                                 max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 300)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 300)          0           flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            301         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,213,401\n",
      "Trainable params: 1,213,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25011 samples, validate on 2780 samples\n",
      "Epoch 1/5\n",
      "25011/25011 [==============================] - 163s 7ms/step - loss: 2.3132 - acc: 0.8549 - val_loss: 2.2423 - val_acc: 0.8594\n",
      "Epoch 2/5\n",
      "25011/25011 [==============================] - 160s 6ms/step - loss: 2.3132 - acc: 0.8549 - val_loss: 2.2423 - val_acc: 0.8594\n",
      "Epoch 3/5\n",
      "25011/25011 [==============================] - 154s 6ms/step - loss: 2.3132 - acc: 0.8549 - val_loss: 2.2423 - val_acc: 0.8594\n",
      "Epoch 4/5\n",
      "25011/25011 [==============================] - 149s 6ms/step - loss: 2.3132 - acc: 0.8549 - val_loss: 2.2423 - val_acc: 0.8594\n",
      "Epoch 5/5\n",
      "25011/25011 [==============================] - 150s 6ms/step - loss: 2.3132 - acc: 0.8549 - val_loss: 2.2423 - val_acc: 0.8594\n",
      "Preparing embedding matrix.\n",
      "Training model.\n",
      "(?, 97, 200)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 97)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 97, 200)      972800      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 97, 200, 1)   0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 93, 1, 100)   100100      reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 94, 1, 100)   80100       reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 95, 1, 100)   60100       reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 1, 1, 100)    0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1, 1, 300)    0           max_pooling2d_22[0][0]           \n",
      "                                                                 max_pooling2d_23[0][0]           \n",
      "                                                                 max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 300)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 300)          0           flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            1204        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,214,304\n",
      "Trainable params: 1,214,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3618 samples, validate on 402 samples\n",
      "Epoch 1/1\n",
      "3618/3618 [==============================] - 25s 7ms/step - loss: 1.2009 - acc: 0.4549 - val_loss: 1.0699 - val_acc: 0.5547\n",
      "trained\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten, Reshape, concatenate, Dropout\n",
    "from keras.layers import  Conv2D, MaxPooling2D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from utils import get_entity_dict, smaller_subtree_containing_the_drugs\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "output_path_name = \"task9.2_ensamble_cascade_90.txt\"\n",
    "\n",
    "output_path = \"evaluations/\" + output_path_name\n",
    "results_path = output_path.replace('.txt', '_All_scores.log')\n",
    "datadir = '../../data/Test-DDI/DrugBank'\n",
    "training_data = '/home/raquel/Documents/mai/ahlt/data/Train/All'\n",
    "train_df_path = '../../../data/DF/train.csv'\n",
    "processed_train_df_path = '../../../data/DF/train_processed.csv'\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder_bin = LabelBinarizer()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "def preprocess(train_df, processed_train_path,encoder, cascade=False):\n",
    "    try:\n",
    "        train_df = pd.read_csv(processed_train_path, index_col=0)\n",
    "        print('loaded preprocessed data')\n",
    "    except:\n",
    "        print('preprocessing data')\n",
    "        for index, row in train_df.iterrows():\n",
    "            # print(train_df.loc[index, 'sentence_text'], train_df.loc[index, ['e1', 'e2']])\n",
    "            new_sentence = smaller_subtree_containing_the_drugs(train_df.loc[index, 'sentence_text'],\n",
    "                                                                train_df.loc[index, ['e1', 'e2']])\n",
    "            train_df.loc[index, 'sentence_text'] = new_sentence\n",
    "        train_df.to_csv(processed_train_path)\n",
    "    sentences_train = train_df.sentence_text.values\n",
    "    y_train = train_df['relation_type'].values\n",
    "    if not cascade:\n",
    "        y_train_encoded = encoder.fit_transform(y_train)\n",
    "    else :\n",
    "        y_train_encoded = y_train\n",
    "\n",
    "    dictionary = {}\n",
    "    for index, row in train_df.iterrows():\n",
    "        d_1 = row['e1'].lower()\n",
    "        d_2 = row['e2'].lower()\n",
    "        interaction = row['relation_type']\n",
    "        if interaction == 'none':\n",
    "            interaction = 'null'\n",
    "        if d_1 not in dictionary:\n",
    "            dictionary[d_1] = {}\n",
    "        if d_2 not in dictionary:\n",
    "            dictionary[d_2] = {}\n",
    "        dictionary[d_1][d_2] = interaction\n",
    "        dictionary[d_2][d_1] = interaction\n",
    "\n",
    "    return sentences_train,dictionary, y_train_encoded\n",
    "\n",
    "\n",
    "def kimCNN(embedding_output_size, imput_size, vocab_size, num_labels=5,loss='categorical_crossentropy'):\n",
    "    \"\"\"\n",
    "    Convolution neural network model for sentence classification.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_output_size: Dimension of the embedding space.\n",
    "    vocab_size: size of the vocabulary\n",
    "    imput_size: number of features of the imput.\n",
    "    num_labels: number of output labels\n",
    "    Returns\n",
    "    -------\n",
    "    compiled keras model\n",
    "    \"\"\"\n",
    "    print('Preparing embedding matrix.')\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                                output_dim=embedding_output_size,\n",
    "                                input_length=imput_size,\n",
    "                                trainable=True)\n",
    "\n",
    "    print('Training model.')\n",
    "\n",
    "    sequence_input = Input(shape=(imput_size,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    print(embedded_sequences.shape)\n",
    "\n",
    "    # add first conv filter\n",
    "    embedded_sequences = Reshape((imput_size, embedding_output_size, 1))(embedded_sequences)\n",
    "    x = Conv2D(100, (5, embedding_output_size), activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling2D((imput_size - 5 + 1, 1))(x)\n",
    "\n",
    "    # add second conv filter.\n",
    "    y = Conv2D(100, (4, embedding_output_size), activation='relu')(embedded_sequences)\n",
    "    y = MaxPooling2D((imput_size - 4 + 1, 1))(y)\n",
    "\n",
    "    # add third conv filter.\n",
    "    z = Conv2D(100, (3, embedding_output_size), activation='relu')(embedded_sequences)\n",
    "    z = MaxPooling2D((imput_size - 3 + 1, 1))(z)\n",
    "\n",
    "    # concate the conv layers\n",
    "    alpha = concatenate([x, y, z])\n",
    "\n",
    "    # flatted the pooled features.\n",
    "    alpha = Flatten()(alpha)\n",
    "\n",
    "    # dropout\n",
    "    alpha = Dropout(0.5)(alpha)\n",
    "\n",
    "    # predictions\n",
    "    preds = Dense(num_labels, activation='softmax')(alpha)\n",
    "\n",
    "    # build model\n",
    "    model = Model(sequence_input, preds)\n",
    "    adadelta = optimizers.Adadelta()\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=adadelta,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_cnn():\n",
    "    train_df = pd.read_csv(train_df_path, index_col=0)\n",
    "    sentences_train, dictionary, y_train = preprocess(train_df, processed_train_df_path, encoder,cascade=True)\n",
    "\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "    max_s = [len(x) for x in X_train]\n",
    "    maxlen = np.max(max_s)\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "    word_embedding_size = 200\n",
    "\n",
    "    # Binary\n",
    "    y_binary = ['none' if i == 'none' else 'interaction' for i in y_train]\n",
    "    y_bin_encoded = encoder_bin.fit_transform(y_binary)\n",
    "\n",
    "    y_train_encoded = encoder.fit_transform(y_train[np.array(y_binary)=='interaction'])\n",
    "\n",
    "    binary_classifier = kimCNN(embedding_output_size=word_embedding_size, imput_size=X_train.shape[1],\n",
    "                               vocab_size=vocab_size,\n",
    "                               num_labels=1,loss='binary_crossentropy')\n",
    "\n",
    "    binary_classifier.fit(X_train, y_bin_encoded,\n",
    "                          epochs=5,\n",
    "                          verbose=True,\n",
    "                          batch_size=100,\n",
    "                          validation_split=0.1,\n",
    "                          class_weight='auto')\n",
    "\n",
    "    classifier = kimCNN(embedding_output_size=word_embedding_size, imput_size=X_train.shape[1], vocab_size=vocab_size,\n",
    "                        num_labels=4)\n",
    "\n",
    "    classifier.fit(X_train[np.array(y_binary)=='interaction',:], y_train_encoded,\n",
    "                   epochs=1,\n",
    "                   verbose=True,\n",
    "                   batch_size=100,\n",
    "                   validation_split=0.1,\n",
    "                   class_weight='auto')\n",
    "\n",
    "    print('trained')\n",
    "    return binary_classifier, classifier, dictionary, maxlen\n",
    "\n",
    "\n",
    "binary_classifier, classifier, dictionary, maxlen = train_cnn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([['none']], dtype='<U11')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentence = 'Fluconazole, an inhibitor of P450 2C9, decreased active metabolite concentration and increased losartan concentration.'\n",
    "e1 = 'Fluconazole'\n",
    "e2= 'losartan'\n",
    "processed_sentence = smaller_subtree_containing_the_drugs(sentence, [e1, e2])\n",
    "sentence_array = tokenizer.texts_to_sequences([processed_sentence])\n",
    "sentence_array = pad_sequences(sentence_array, padding='post', maxlen=maxlen)\n",
    "# print(sentence_array)\n",
    "y_bin = binary_classifier.predict(sentence_array)\n",
    "y_class = y_bin > 0.5\n",
    "y_class = y_class.astype(int)\n",
    "y_label = [encoder_bin.classes_[y_class] for l in y_class]\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22598241, 0.5026155 , 0.04867313, 0.222729  ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs = classifier.predict(sentence_array)\n",
    "y_class = np.argmax(y_probs, axis=1)\n",
    "y_pred = encoder.classes_[y_class]\n",
    "\n",
    "y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'null')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_interaction(sentence):\n",
    "    # uses the vectorizer and the classifier already trained\n",
    "    sentence_array = tokenizer.texts_to_sequences([sentence])\n",
    "    sentence_array = pad_sequences(sentence_array, padding='post', maxlen=maxlen)\n",
    "    # print(sentence_array)\n",
    "    y_bin = binary_classifier.predict(sentence_array)\n",
    "    if y_bin[0] > 0.6:\n",
    "        return False, \"null\"\n",
    "    else:\n",
    "        y_probs = classifier.predict(sentence_array)\n",
    "        y_class = np.argmax(y_probs, axis=1)\n",
    "        y_pred = encoder.classes_[y_class]\n",
    "        return True, y_pred[0]\n",
    "\n",
    "check_interaction(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
