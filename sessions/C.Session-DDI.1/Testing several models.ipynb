{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from xml.dom.minidom import parse\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "output_path_name = \"task9.2_raquel_50.txt\"\n",
    "\n",
    "output_path = \"evaluations/\" + output_path_name\n",
    "results_path = output_path.replace('.txt', '_All_scores.log')\n",
    "datadir = '../../data/Test-DDI/DrugBank'\n",
    "training_data = '/home/raquel/Documents/mai/ahlt/data/Train/All'\n",
    "train_df_path = '/home/raquel/Documents/mai/ahlt/data/DF/train.csv'\n",
    "test_df_path = '/home/raquel/Documents/mai/ahlt/data/DF/test.csv'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_dense(model=LogisticRegression()):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f1_score(y_test, y_pred, average=None))\n",
    "    print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "    print(recall_score(y_test, y_pred, average=\"macro\"))    \n",
    "    \n",
    "def classify_dense(model=LogisticRegression()):\n",
    "    model.fit(X_train.toarray(), y_train)\n",
    "    y_pred = model.predict(X_test.toarray())\n",
    "\n",
    "    print(f1_score(y_test, y_pred, average=None))\n",
    "    print(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "    print(recall_score(y_test, y_pred, average=\"macro\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43436754 0.29069767 0.         0.11494253 0.90380981]\n",
      "0.44594754325836866\n",
      "0.32907380287254506\n"
     ]
    }
   ],
   "source": [
    "def baseline_nn():\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def classify_keras(model=baseline_nn()):\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    y_train_encoded = encoder.fit_transform(y_train)\n",
    "    y_test_encoded = encoder.fit_transform(y_test)\n",
    "    model.fit(X_train, y_train_encoded,\n",
    "                    epochs=10,\n",
    "                    verbose=False,\n",
    "                    batch_size=10)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_class = y_pred.argmax(axis=-1)\n",
    "    y_labels = [encoder.classes_[l] for l in y_class]\n",
    "    print(f1_score(y_test, y_labels, average=None))\n",
    "    print(precision_score(y_test, y_labels, average=\"macro\"))\n",
    "    print(recall_score(y_test, y_labels, average=\"macro\"))    \n",
    "classify_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27791, 5938)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_data(train_df,test_df):\n",
    "    \n",
    "    sentences_train = train_df.sentence_text.values\n",
    "    sentences_test = test_df.sentence_text.values\n",
    "    \n",
    "    y_train = train_df['relation_type'].values\n",
    "    y_test = test_df['relation_type'].values\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_train)\n",
    "    \n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test =  vectorizer.transform(sentences_test)\n",
    "   \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorize_data(train_df,test_df)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22093023 0.20728745 0.         0.20560748 0.76413302]\n",
      "0.2664544794420739\n",
      "0.3412766669715575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf3 = GaussianNB()\n",
    "classify_dense(clf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37368421 0.2622549  0.         0.18032787 0.86800663]\n",
      "0.3540076109181062\n",
      "0.32969990441305863\n"
     ]
    }
   ],
   "source": [
    "classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36847104 0.33685601 0.03773585 0.34810637 0.69007715]\n",
      "0.36338671275714607\n",
      "0.5739660955602753\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=30,n_jobs=-1,\n",
    "                             class_weight='balanced', random_state=0)\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41438849 0.34820647 0.         0.35674157 0.82390438]\n",
      "0.35234520149448106\n",
      "0.4777753868864936\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=700, max_depth=60,n_jobs=-1,\n",
    "                             class_weight='balanced', random_state=0)\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39940828 0.33965844 0.         0.34343434 0.8397692 ]\n",
      "0.35781792721459327\n",
      "0.4489296224567484\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=700, max_depth=80,n_jobs=-1,\n",
    "                             class_weight='balanced', random_state=0)\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42159383 0.32683658 0.02020202 0.25174825 0.88098918]\n",
      "0.4607302688813806\n",
      "0.3650231364244492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10,5),learning_rate='adaptive')\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42761693 0.34396671 0.         0.24230769 0.882853  ]\n",
      "0.3841840390116512\n",
      "0.3772648009290561\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=700, max_depth=60,n_jobs=-1,\n",
    "                             class_weight='balanced', random_state=0)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,5),learning_rate='adaptive')\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble=VotingClassifier(estimators=[('Random Forest', rf), ('MLP', mlp)], \n",
    "                       voting='soft')\n",
    "classify(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7883861050632311\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = vectorize_data(train_df,test_df)\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "\n",
    "svd.fit(X_train)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "X_train = svd.transform(X_train)  \n",
    "\n",
    "X_test = svd.transform(X_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36847104 0.33685601 0.03773585 0.34810637 0.69007715]\n",
      "0.36338671275714607\n",
      "0.5739660955602753\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=30,n_jobs=-1,\n",
    "                             class_weight='balanced', random_state=0)\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30536913 0.22955264 0.02083333 0.31319911 0.47318312]\n",
      "0.29864652225939814\n",
      "0.5161744094383395\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma='auto',class_weight='balanced')\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30950378 0.24197745 0.0265252  0.32951289 0.44484101]\n",
      "0.304448349777018\n",
      "0.5421351217408685\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma='auto',class_weight='balanced',C=0.5)\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_df_path, index_col=0)\n",
    "test_df = pd.read_csv(test_df_path, index_col=0)\n",
    "\n",
    "\n",
    "def tokenize_data(train_df,test_df):\n",
    "    \n",
    "    sentences_train = train_df.sentence_text.values\n",
    "    sentences_test = test_df.sentence_text.values\n",
    "    \n",
    "    y_train = train_df['relation_type'].values\n",
    "    y_test = test_df['relation_type'].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    \n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 \n",
    "    \n",
    "    \n",
    "    maxlen = 90\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = tokenize_data(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(x) for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.90052961]\n",
      "0.16558073654390934\n",
      "0.19742453029343465\n"
     ]
    }
   ],
   "source": [
    "classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.121673   0.08962264 0.         0.06153846 0.90507555]\n",
      "0.3897673890160573\n",
      "0.22891335715488448\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=30,n_jobs=-1,\n",
    "                             class_weight='balanced', random_state=0)\n",
    "classify(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e70a9085da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8e70a9085da8>\u001b[0m in \u001b[0;36mtrain_baseline\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msentences_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relation_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "test_df = \n",
    "\n",
    "def train_baseline():\n",
    "    train_df = pd.read_csv(train_df_path, index_col=0)\n",
    "    sentences_train = train_df.sentence_text.values\n",
    "    y_train = train_df['relation_type'].values\n",
    "    vectorizer = Tokenizer(num_words=5000)\n",
    "    vectorizer.fit_on_texts(sentences_train)\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     vectorizer.fit(sentences_train)\n",
    "#     X_train = vectorizer.transform(sentences_train)\n",
    "    \n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    \n",
    "    print('training...')\n",
    "    # classifier = RandomForestClassifier(n_jobs=-1, class_weight='balanced')\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print('trained')\n",
    "    return vectorizer, classifier\n",
    "\n",
    "\n",
    "vectorizer, classifier = train_baseline()\n",
    "\n",
    "\n",
    "def check_interaction(sentence):\n",
    "    # uses the vectorizer and the classifier already trained\n",
    "    sentence_array = vectorizer.transform([sentence])\n",
    "    y_pred = classifier.predict(sentence_array)\n",
    "\n",
    "    if y_pred[0] == 'none':\n",
    "        return False, \"null\"\n",
    "    else:\n",
    "        return True, y_pred[0]\n",
    "\n",
    "\n",
    "def predict(datadir, output_path, test=False):\n",
    "    # process each file in directory\n",
    "    with open(output_path, 'w') as file:\n",
    "        for f in tqdm(listdir(datadir)):\n",
    "\n",
    "            # parse XML file, obtaining a DOM tree\n",
    "            tree = parse(datadir + \"/\" + f)\n",
    "\n",
    "            # process each sentence in the file\n",
    "            sentences = tree.getElementsByTagName(\"sentence\")\n",
    "            for s in sentences:\n",
    "                sid = s.attributes[\"id\"].value  # get sentence id\n",
    "                stext = s.attributes[\"text\"].value  # get sentence text\n",
    "\n",
    "                # load sentence entities\n",
    "                entities = {}\n",
    "                ents = s.getElementsByTagName(\"entity\")\n",
    "                for e in ents:\n",
    "                    id = e.attributes[\"id\"].value\n",
    "                    offs = e.attributes[\"charOffset\"].value.split(\"-\")\n",
    "                    entities[id] = offs\n",
    "\n",
    "                # for each pair in the sentence, decide whether it is DDI and its type\n",
    "                pairs = s.getElementsByTagName(\"pair\")\n",
    "                for p in pairs:\n",
    "                    id_e1 = p.attributes[\"e1\"].value\n",
    "                    id_e2 = p.attributes[\"e2\"].value\n",
    "                    (is_ddi, ddi_type) = check_interaction(stext)\n",
    "                    ddi = \"1\" if is_ddi else \"0\"\n",
    "                    file.write(sid + \"|\" + id_e1 + \"|\" + id_e2 + \"|\" + ddi + \"|\" + ddi_type)\n",
    "                    file.write('\\n')\n",
    "                    if test:\n",
    "                        return\n",
    "\n",
    "\n",
    "def show_results(results_path):\n",
    "    import subprocess\n",
    "    subprocess.call(['java', '-jar', '../../eval/evaluateDDI.jar', '../../data/Test-DDI/All', output_path])\n",
    "    results_file = open(results_path, 'r')\n",
    "    print(results_file.read())\n",
    "    results_file.close()\n",
    "\n",
    "\n",
    "predict(datadir, output_path, False)\n",
    "show_results(results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahlt",
   "language": "python",
   "name": "ahlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
